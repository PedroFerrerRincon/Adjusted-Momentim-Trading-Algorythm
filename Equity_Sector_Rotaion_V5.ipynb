{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PedroFerrerRincon/Adjusted-Momentim-Trading-Algorythm/blob/main/Equity_Sector_Rotaion_V5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To do : low epochs 2000 first neuron"
      ],
      "metadata": {
        "id": "P7yOyRoWlRiw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgAJImlcqiyw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_j8rlb-95MMX"
      },
      "source": [
        "V2 Changes - Added \"Earnings Growth Q-1\", \"Earnings Growth Q-2\", \"Financing Gap\" - Added oversampling drwdowns - removed custom loss function - created six folder system - added excel summary - added window variable - validation from 2022\n",
        "\n",
        "V3 Changes - Added hyperparameter combination\n",
        "\n",
        "V4 Changes - Added Skewness, Kurtosis, Beta and Returns Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "5vz3uw_K44Ll",
        "outputId": "8eb8253c-e370-4ecd-d89c-c35f1b3d6949"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 27958 because val_loss (0.050579) exceeded 105% of best_val_loss (0.047528).\n",
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "All results have been zipped into: grid_search_results_LSTM.zip\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0a8d7c7a-30be-45c3-b395-90ab274eedf0\", \"grid_search_results_LSTM.zip\", 817971687)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All hyperparameter combinations are complete.\n",
            "Master summary saved to: grid_search_results_LSTM/grid_search_summary.xlsx\n",
            "Zipped folder created at: grid_search_results_LSTM.zip\n",
            "Script complete.\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import random\n",
        "import os\n",
        "import shutil  # For copying the checkpoint file\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import zipfile\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# For reproducibility\n",
        "RANDOM_STATE = 42\n",
        "def set_random_seeds(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "set_random_seeds(RANDOM_STATE)\n",
        "\n",
        "# ---------------------------------\n",
        "# 2) User Configuration\n",
        "# ---------------------------------\n",
        "FILE_NAME = 'AllDataTraining.csv'\n",
        "\n",
        "# The equity price/index columns we want to predict 180-step fwd returns on:\n",
        "SECTOR_PRICE_COLS = [\n",
        "    \"S&P GL 1200 Energy\",\n",
        "    \"S&P GL 1200 Financials\",\n",
        "    \"S&P GL 1200 Health Care\",\n",
        "    \"S&P GL 1200 Materials\",\n",
        "    \"S&P GL 1200 Industrials\",\n",
        "    \"S&P GL 1200 Consumer Staples\",\n",
        "    \"S&P GL 1200 Utilities\",\n",
        "    \"S&P GL 1200 Consumer Discretionary\",\n",
        "    \"S&P GL 1200 Communication Services\",\n",
        "    \"S&P GL 1200 Information Technology\"\n",
        "]\n",
        "\n",
        "WINDOW = 180\n",
        "\n",
        "# Benchmark price/index column:\n",
        "BENCHMARK_PRICE_COL = \"SP500\"\n",
        "\n",
        "# Input features for the LSTM:\n",
        "INPUT_COLS = [\n",
        "    'CPI', 'Unemployment', 'GDP Nominal', 'GDP Real',\n",
        "    'M2 Velocity', 'M2', 'Oil Prices USD', 'Credit Spreads',\n",
        "    '1 Mo', '3 Mo', '6 Mo', '1 Yr', '2 Yr', '3 Yr',\n",
        "    '5 Yr', '7 Yr', '10 Yr', '20 Yr', '30 Yr',\n",
        "    'Baltic Dry Index', 'ISM',\n",
        "    '1-3y t-60', '1-3y t-180', '4-7y t-60', '4-7y t-180',\n",
        "    '7-10y t-60', '7-10y t-180',\n",
        "    \"SNP500 t-60\", \"SNP500 t-180\", \"SNP500 VOL30\",\n",
        "    \"EURUSD\", \"EURUSD t-30\",\"EURUSD t-60\",\n",
        "    'Drawdown Spread', '1-3yr Drawdown', '7-10yr Drawdown',\n",
        "]\n",
        "\n",
        "# Candidate lag sets for the LSTM:\n",
        "LAGS_GRID = [\n",
        "    [0, 5, 21, 64, 255],  # example lags\n",
        "]\n",
        "\n",
        "# Hyperparameters: now we will iterate over all elements in these lists\n",
        "EPOCHS          = [100000]\n",
        "BATCH_SIZE      = [1028]\n",
        "DROPOUT         = [0]\n",
        "LEARNING_RATE   = [0.0001]\n",
        "REG_LAMBDA      = [0]\n",
        "NUM_LSTM_LAYERS = [4]\n",
        "FIRST_LAYER_SIZE= [1500]\n",
        "\n",
        "# Train/Validation date ranges:\n",
        "TRAIN1_START = pd.to_datetime(\"1994-12-30\")\n",
        "TRAIN1_END   = pd.to_datetime(\"2000-12-29\")\n",
        "VALIDATION_START = pd.to_datetime(\"2001-01-02\")\n",
        "VALIDATION_END   = pd.to_datetime(\"2010-12-31\")\n",
        "TRAIN2_START = pd.to_datetime(\"2011-01-03\")\n",
        "TRAIN2_END   = pd.to_datetime(\"2021-12-17\")\n",
        "\n",
        "# Rebalancing frequency (days)\n",
        "REBALANCE_FREQUENCY_DAYS = 5\n",
        "\n",
        "# Output directory\n",
        "MAIN_OUTPUT_DIR = \"grid_search_results_LSTM\"\n",
        "os.makedirs(MAIN_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Additional subfolder names (these will live within each combo directory)\n",
        "SUBFOLDER_25pct_2001_2011       = \"portfolio_25pct_2001_2011\"\n",
        "SUBFOLDER_cascading_2001_2011   = \"portfolio_cascading_2001_2011\"\n",
        "SUBFOLDER_25pct_2023_2024       = \"portfolio_25pct_2023_2024\"\n",
        "SUBFOLDER_cascading_2023_2024   = \"portfolio_cascading_2023_2024\"\n",
        "SUBFOLDER_sectors_2001_2011     = \"sector_predictions_2001_2011\"\n",
        "SUBFOLDER_sectors_2023_2024     = \"sector_predictions_2023_2024\"\n",
        "\n",
        "# ---------------------------------\n",
        "# 3) Data Loading and Functions\n",
        "# ---------------------------------\n",
        "def load_data(csv_path: str) -> pd.DataFrame:\n",
        "    \"\"\"Loads CSV with 'Date' column, sets Date as index (datetime).\"\"\"\n",
        "    df = pd.read_csv(csv_path, parse_dates=['Date'], dayfirst=True, low_memory=False)\n",
        "    df.set_index('Date', inplace=True)\n",
        "    df.sort_index(inplace=True)\n",
        "    df = df[df.index.weekday < 5]  # Keep only Mon-Fri if that was intended\n",
        "    return df\n",
        "\n",
        "def add_daily_return_columns(df: pd.DataFrame, price_cols: list) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    For each price/index column in price_cols, create col+\"_daily_ret\" = daily returns.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    for col in price_cols:\n",
        "        daily_ret_col = col + \"_daily_ret\"\n",
        "        df[daily_ret_col] = df[col].pct_change(fill_method=None)\n",
        "    return df\n",
        "\n",
        "def create_180d_forward_returns(df: pd.DataFrame, daily_return_cols: list, window=180) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    For each *daily return* column in daily_return_cols, compute the 180-day forward return.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    n = len(df)\n",
        "    for col in daily_return_cols:\n",
        "        fwd_name = col + \"_fwd180\"\n",
        "        fwd_vals = [np.nan] * n\n",
        "        for i in range(n - window):\n",
        "            window_returns = df[col].iloc[i+1 : i+1+window]\n",
        "            if window_returns.isnull().any():\n",
        "                fwd_vals[i] = np.nan\n",
        "            else:\n",
        "                fwd_vals[i] = np.prod(1.0 + window_returns.values) - 1.0\n",
        "        df[fwd_name] = fwd_vals\n",
        "    return df\n",
        "\n",
        "def split_three_periods_precomputed_data(X, y, dates,\n",
        "                                        train1_start, train1_end,\n",
        "                                        validation_start, validation_end,\n",
        "                                        train2_start, train2_end):\n",
        "    \"\"\"\n",
        "    Split (X, y) into train (train1 + train2) and validation sets.\n",
        "    \"\"\"\n",
        "    dates = pd.to_datetime(dates)\n",
        "    train1_idx = [i for i, d in enumerate(dates) if train1_start <= d <= train1_end]\n",
        "    valid_idx  = [i for i, d in enumerate(dates) if validation_start <= d <= validation_end]\n",
        "    train2_idx = [i for i, d in enumerate(dates) if train2_start <= d <= train2_end]\n",
        "\n",
        "    if len(train1_idx) == 0 or len(train2_idx) == 0 or len(valid_idx) == 0:\n",
        "        raise ValueError(\"One or more split periods are empty. Check your date ranges.\")\n",
        "\n",
        "    X_train = np.concatenate((X[train1_idx], X[train2_idx]), axis=0)\n",
        "    y_train = np.concatenate((y[train1_idx], y[train2_idx]), axis=0)\n",
        "    train_dates = [dates[i] for i in (train1_idx + train2_idx)]\n",
        "\n",
        "    X_eval = X[valid_idx]\n",
        "    y_eval = y[valid_idx]\n",
        "    eval_dates = [dates[i] for i in valid_idx]\n",
        "\n",
        "    return (X_train, y_train, train_dates), (X_eval, y_eval, eval_dates)\n",
        "\n",
        "def prepare_lagged_data(df: pd.DataFrame, input_cols: list, label_cols: list, lags: list):\n",
        "    \"\"\"\n",
        "    Create X, y, valid_dates for an LSTM with the given lags.\n",
        "    X.shape = (samples, len(lags), len(input_cols))\n",
        "    y.shape = (samples, len(label_cols))\n",
        "    \"\"\"\n",
        "    df = df.sort_index()\n",
        "    X_list, y_list, valid_dates = [], [], []\n",
        "    all_dates = df.index.unique().sort_values()\n",
        "\n",
        "    for i, current_date in enumerate(all_dates):\n",
        "        lag_data = []\n",
        "        missing_lag = False\n",
        "        for lag in lags:\n",
        "            lag_idx = i - lag\n",
        "            if lag_idx < 0:\n",
        "                missing_lag = True\n",
        "                break\n",
        "            lag_date = all_dates[lag_idx]\n",
        "            if lag_date not in df.index:\n",
        "                missing_lag = True\n",
        "                break\n",
        "            row = df.loc[lag_date, input_cols]\n",
        "            if isinstance(row, pd.DataFrame):\n",
        "                row = row.iloc[0]\n",
        "            if row.isnull().any():\n",
        "                missing_lag = True\n",
        "                break\n",
        "            lag_data.append(row.values.astype(float))\n",
        "        if missing_lag or (current_date not in df.index):\n",
        "            continue\n",
        "\n",
        "        label_val = df.loc[current_date, label_cols]\n",
        "        if isinstance(label_val, pd.DataFrame):\n",
        "            label_val = label_val.iloc[0]\n",
        "        if label_val.isnull().any():\n",
        "            continue\n",
        "\n",
        "        X_list.append(np.array(lag_data))\n",
        "        y_list.append(label_val.values.astype(float))\n",
        "        valid_dates.append(current_date)\n",
        "\n",
        "    X_arr = np.array(X_list)\n",
        "    y_arr = np.array(y_list)\n",
        "    return X_arr, y_arr, valid_dates\n",
        "\n",
        "# ---------------------------------\n",
        "# 4) Model and Training\n",
        "# ---------------------------------\n",
        "def build_lstm_model(n_lags, n_features, n_outputs,\n",
        "                     num_lstm_layers=2, first_layer_size=64,\n",
        "                     dropout_rate=0.0, reg_lambda=0.0,\n",
        "                     learning_rate=0.001):\n",
        "    \"\"\"\n",
        "    Build a multi-layer LSTM with halving layer sizes, culminating in Dense(n_outputs).\n",
        "    No custom loss; just 'mse'.\n",
        "    \"\"\"\n",
        "    layer_sizes = [first_layer_size]\n",
        "    for _ in range(1, num_lstm_layers):\n",
        "        layer_sizes.append(max(1, layer_sizes[-1]//2))\n",
        "\n",
        "    l2_reg = regularizers.l2(reg_lambda)\n",
        "    model = Sequential()\n",
        "    for i, units in enumerate(layer_sizes):\n",
        "        return_seq = (i < num_lstm_layers - 1)\n",
        "        if i == 0:\n",
        "            model.add(\n",
        "                LSTM(units,\n",
        "                     return_sequences=return_seq,\n",
        "                     input_shape=(n_lags, n_features),\n",
        "                     kernel_regularizer=l2_reg)\n",
        "            )\n",
        "        else:\n",
        "            model.add(\n",
        "                LSTM(units,\n",
        "                     return_sequences=return_seq,\n",
        "                     kernel_regularizer=l2_reg)\n",
        "            )\n",
        "        if dropout_rate > 0:\n",
        "            model.add(Dropout(dropout_rate))\n",
        "\n",
        "    model.add(Dense(n_outputs, kernel_regularizer=l2_reg))\n",
        "\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(loss='mse', optimizer=optimizer, metrics=['mse'])\n",
        "    return model\n",
        "\n",
        "# ---------------------------------\n",
        "# 4B) Custom Callback for Post-15000 Behavior\n",
        "# ---------------------------------\n",
        "class Post15000EarlyStopping(tf.keras.callbacks.Callback):\n",
        "    \"\"\"\n",
        "    After epoch 15,000, if the current validation loss is more than 5% above\n",
        "    the best (lowest) val_loss (which we set starting from epoch 15,000),\n",
        "    we stop training. Otherwise, if val_loss sets a new low, we update\n",
        "    best_val_loss and save the model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, best_model_path=\"temp_best_model.keras\"):\n",
        "        super().__init__()\n",
        "        self.best_val_loss = None\n",
        "        self.best_model_path = best_model_path\n",
        "        self.baseline_set = False   # Will become True at epoch == 15000\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        val_loss = logs.get('val_loss')\n",
        "        if val_loss is None:\n",
        "            return\n",
        "\n",
        "        current_epoch = epoch + 1\n",
        "        # Ignore everything before epoch 15,000\n",
        "        if current_epoch < 15000:\n",
        "            return\n",
        "\n",
        "        # At epoch 15,000, set the baseline (best_val_loss = current val_loss)\n",
        "        if current_epoch == 15000 and not self.baseline_set:\n",
        "            self.best_val_loss = val_loss\n",
        "            self.baseline_set = True\n",
        "            # Save model at epoch 15,000 (baseline checkpoint)\n",
        "            self.model.save(self.best_model_path)\n",
        "            return\n",
        "\n",
        "        # Beyond epoch 15,000\n",
        "        if not self.baseline_set:\n",
        "            # If for some reason we missed it, we set baseline now\n",
        "            self.best_val_loss = val_loss\n",
        "            self.baseline_set = True\n",
        "            self.model.save(self.best_model_path)\n",
        "            return\n",
        "\n",
        "        # If val_loss is < best_val_loss, update\n",
        "        if val_loss < self.best_val_loss:\n",
        "            self.best_val_loss = val_loss\n",
        "            self.model.save(self.best_model_path)\n",
        "        else:\n",
        "            # If val_loss > 1.05 * self.best_val_loss => STOP\n",
        "            if val_loss > 1.05 * self.best_val_loss:\n",
        "                print(\n",
        "                    f\"Early stopping at epoch {current_epoch} because val_loss \"\n",
        "                    f\"({val_loss:.6f}) exceeded 105% of best_val_loss \"\n",
        "                    f\"({self.best_val_loss:.6f}).\"\n",
        "                )\n",
        "                self.model.stop_training = True\n",
        "\n",
        "# ---------------------------------\n",
        "# 5) Backtesting + Heatmaps\n",
        "# ---------------------------------\n",
        "def backtest_top4_portfolio(dates_eval, y_pred, df_eval,\n",
        "                            sector_daily_ret_cols, benchmark_daily_ret_col,\n",
        "                            rebal_freq=5):\n",
        "    \"\"\"\n",
        "    We do rebalancing every 'rebal_freq' days for a 25%-each approach:\n",
        "      - On rebal day i, pick top-4 predicted from y_pred[i-1], each 25%.\n",
        "    \"\"\"\n",
        "    df_backtest = df_eval.loc[dates_eval, sector_daily_ret_cols + [benchmark_daily_ret_col]].copy()\n",
        "    df_backtest.sort_index(inplace=True)\n",
        "\n",
        "    n = len(df_backtest)\n",
        "    strategy_nav = np.ones(n)\n",
        "    benchmark_nav = np.ones(n)\n",
        "    num_sectors = len(sector_daily_ret_cols)\n",
        "    top4_matrix = np.zeros((n, num_sectors), dtype=int)\n",
        "\n",
        "    current_top4 = []\n",
        "    for i in range(n):\n",
        "        if i == 0 or (i % rebal_freq == 0):\n",
        "            # On rebal day, pick top-4 from y_pred[i-1], but i-1 must be valid\n",
        "            if i == 0:\n",
        "                current_top4 = []\n",
        "            else:\n",
        "                pred_prev = y_pred[i - 1]\n",
        "                top4 = np.argsort(pred_prev)[-4:]\n",
        "                current_top4 = top4\n",
        "\n",
        "        day_returns = df_backtest[sector_daily_ret_cols].iloc[i].values\n",
        "        if len(current_top4) > 0:\n",
        "            top4_matrix[i, current_top4] = 1\n",
        "            strat_daily_return = np.mean(day_returns[current_top4])\n",
        "        else:\n",
        "            strat_daily_return = 0.0\n",
        "\n",
        "        if i > 0:\n",
        "            strategy_nav[i] = strategy_nav[i-1] * (1.0 + strat_daily_return)\n",
        "        else:\n",
        "            strategy_nav[i] = 1.0 * (1.0 + strat_daily_return)\n",
        "\n",
        "        bm_ret = df_backtest[benchmark_daily_ret_col].iloc[i]\n",
        "        if i > 0:\n",
        "            benchmark_nav[i] = benchmark_nav[i-1] * (1.0 + bm_ret)\n",
        "        else:\n",
        "            benchmark_nav[i] = 1.0 * (1.0 + bm_ret)\n",
        "\n",
        "    nav_df = pd.DataFrame({\n",
        "        \"Strategy_NAV\": strategy_nav,\n",
        "        \"Benchmark_NAV\": benchmark_nav\n",
        "    }, index=df_backtest.index)\n",
        "    return nav_df, top4_matrix\n",
        "\n",
        "def backtest_top4_cascading_weights(dates_eval, y_pred, df_eval,\n",
        "                                    sector_daily_ret_cols, benchmark_daily_ret_col,\n",
        "                                    rebal_freq=5):\n",
        "    \"\"\"\n",
        "    A backtest that applies weights [0.4, 0.3, 0.2, 0.1] to the top 4 predicted.\n",
        "    \"\"\"\n",
        "    df_backtest = df_eval.loc[dates_eval, sector_daily_ret_cols + [benchmark_daily_ret_col]].copy()\n",
        "    df_backtest.sort_index(inplace=True)\n",
        "\n",
        "    n = len(df_backtest)\n",
        "    strategy_nav = np.ones(n)\n",
        "    benchmark_nav = np.ones(n)\n",
        "    num_sectors = len(sector_daily_ret_cols)\n",
        "\n",
        "    daily_weights = np.zeros((n, num_sectors), dtype=float)\n",
        "    top_weights = [0.4, 0.3, 0.2, 0.1]\n",
        "    current_weights = np.zeros(num_sectors, dtype=float)\n",
        "\n",
        "    for i in range(n):\n",
        "        if i == 0 or (i % rebal_freq == 0):\n",
        "            # On rebal day, pick top-4 from y_pred[i-1]\n",
        "            if i == 0:\n",
        "                current_weights = np.zeros(num_sectors)\n",
        "            else:\n",
        "                pred_prev = y_pred[i - 1]\n",
        "                sorted_idx = np.argsort(pred_prev)[::-1]  # descending\n",
        "                top4 = sorted_idx[:4]\n",
        "                w_new = np.zeros(num_sectors)\n",
        "                for rank, sec_idx in enumerate(top4):\n",
        "                    w_new[sec_idx] = top_weights[rank]\n",
        "                current_weights = w_new\n",
        "\n",
        "        daily_weights[i] = current_weights\n",
        "        day_returns = df_backtest[sector_daily_ret_cols].iloc[i].values\n",
        "        strat_daily_return = np.dot(current_weights, day_returns)\n",
        "\n",
        "        if i > 0:\n",
        "            strategy_nav[i] = strategy_nav[i - 1] * (1.0 + strat_daily_return)\n",
        "        else:\n",
        "            strategy_nav[i] = 1.0 * (1.0 + strat_daily_return)\n",
        "\n",
        "        bm_ret = df_backtest[benchmark_daily_ret_col].iloc[i]\n",
        "        if i > 0:\n",
        "            benchmark_nav[i] = benchmark_nav[i - 1] * (1.0 + bm_ret)\n",
        "        else:\n",
        "            benchmark_nav[i] = 1.0 * (1.0 + bm_ret)\n",
        "\n",
        "    nav_df = pd.DataFrame({\n",
        "        \"Strategy_NAV\": strategy_nav,\n",
        "        \"Benchmark_NAV\": benchmark_nav\n",
        "    }, index=df_backtest.index)\n",
        "    return nav_df, daily_weights\n",
        "\n",
        "def compute_performance(nav_series: pd.Series):\n",
        "    \"\"\"\n",
        "    Return dict of performance stats: total return, CAGR, vol, sharpe, max drawdown.\n",
        "    (Skew/Kurt/Beta are added below, in the main script.)\n",
        "    \"\"\"\n",
        "    s = nav_series.copy().dropna()\n",
        "    if len(s) < 2:\n",
        "        return {}\n",
        "    total_return = s.iloc[-1] / s.iloc[0] - 1\n",
        "    days = (s.index[-1] - s.index[0]).days\n",
        "    years = days / 365.25 if days > 0 else 1e-9\n",
        "    cagr = (1 + total_return)**(1/years) - 1 if years>0 else np.nan\n",
        "    daily_ret = s.pct_change().dropna()\n",
        "    vol = daily_ret.std() * np.sqrt(252) if len(daily_ret) > 1 else np.nan\n",
        "    sharpe = 0\n",
        "    if daily_ret.std() and daily_ret.std() != 0:\n",
        "        sharpe = (daily_ret.mean() / daily_ret.std()) * np.sqrt(252)\n",
        "    dd = (s / s.cummax()) - 1\n",
        "    max_dd = dd.min()\n",
        "    return {\n",
        "        \"Total_Return\": total_return,\n",
        "        \"CAGR\": cagr,\n",
        "        \"Volatility\": vol,\n",
        "        \"Sharpe_Ratio\": sharpe,\n",
        "        \"Max_Drawdown_pct\": max_dd * 100\n",
        "    }\n",
        "\n",
        "def plot_drawdowns(nav_df, output_path):\n",
        "    \"\"\"\n",
        "    Plot drawdown lines for Strategy and Benchmark.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10,5))\n",
        "    s_strat = nav_df[\"Strategy_NAV\"]\n",
        "    dd_strat = s_strat / s_strat.cummax() - 1\n",
        "\n",
        "    s_bench = nav_df[\"Benchmark_NAV\"]\n",
        "    dd_bench = s_bench / s_bench.cummax() - 1\n",
        "\n",
        "    plt.plot(dd_strat.index, dd_strat, label=\"Strategy\")\n",
        "    plt.plot(dd_bench.index, dd_bench, label=\"Benchmark\")\n",
        "    plt.title(\"Drawdowns\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    ax = plt.gca()\n",
        "    ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
        "    ax.xaxis.set_major_formatter(mdates.ConciseDateFormatter(ax.xaxis.get_major_locator()))\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_path)\n",
        "    plt.close()\n",
        "\n",
        "def plot_top4_heatmap(dates, top4_matrix, sector_labels, output_path):\n",
        "    \"\"\"\n",
        "    Heatmap for the 25%-each approach (1=selected, 0=not).\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(12,6))\n",
        "    cax = ax.imshow(top4_matrix.T, aspect='auto', interpolation='nearest', origin='lower')\n",
        "    ax.set_yticks(range(len(sector_labels)))\n",
        "    ax.set_yticklabels(sector_labels)\n",
        "    step = max(1, len(dates)//10)\n",
        "    ax.set_xticks(range(0, len(dates), step))\n",
        "    xlabels = [dates[i].strftime('%Y-%m-%d') for i in range(0,len(dates), step)]\n",
        "    ax.set_xticklabels(xlabels, rotation=45, ha='right')\n",
        "    fig.colorbar(cax, ax=ax, label=\"1=In Top 4, 0=Not in Top 4\")\n",
        "    ax.set_title(\"Heatmap: 25%-each-of-top-4\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_path)\n",
        "    plt.close()\n",
        "\n",
        "def plot_cascading_weight_heatmap(dates, weight_matrix, sector_labels, output_path):\n",
        "    \"\"\"\n",
        "    Heatmap for the cascading approach: color scale is weight (0.0 to 0.4).\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(12,6))\n",
        "    cax = ax.imshow(weight_matrix.T, aspect='auto', interpolation='nearest', origin='lower')\n",
        "    ax.set_yticks(range(len(sector_labels)))\n",
        "    ax.set_yticklabels(sector_labels)\n",
        "    step = max(1, len(dates)//10)\n",
        "    ax.set_xticks(range(0, len(dates), step))\n",
        "    xlabels = [dates[i].strftime('%Y-%m-%d') for i in range(0,len(dates), step)]\n",
        "    ax.set_xticklabels(xlabels, rotation=45, ha='right')\n",
        "    fig.colorbar(cax, ax=ax, label=\"Sector Weight\")\n",
        "    ax.set_title(\"Heatmap: Cascading Weights\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_path)\n",
        "    plt.close()\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Side-by-side normal frequency bar chart\n",
        "# -----------------------------------------------------------------------------\n",
        "def plot_side_by_side_hist(strategy_rets, bench_rets, title, output_path, bins=50):\n",
        "    \"\"\"\n",
        "    Plots a normal frequency histogram of daily returns for both strategy and benchmark\n",
        "    side-by-side (two bars per bin).\n",
        "    \"\"\"\n",
        "    # Determine a common range for bins based on min/max of both series\n",
        "    min_val = min(strategy_rets.min(), bench_rets.min())\n",
        "    max_val = max(strategy_rets.max(), bench_rets.max())\n",
        "\n",
        "    # Generate bin edges\n",
        "    bin_edges = np.linspace(min_val, max_val, bins + 1)\n",
        "\n",
        "    # Compute hist data for each\n",
        "    hist_strat, _ = np.histogram(strategy_rets, bins=bin_edges)\n",
        "    hist_bench, _ = np.histogram(bench_rets, bins=bin_edges)\n",
        "\n",
        "    # Convert bin edges to centers\n",
        "    bin_centers = 0.5 * (bin_edges[:-1] + bin_edges[1:])\n",
        "\n",
        "    # Width of each bar\n",
        "    width = (bin_edges[1] - bin_edges[0]) * 0.4  # 40% of the bin width => small gap\n",
        "\n",
        "    plt.figure(figsize=(8,5))\n",
        "    # Shift the Strategy bars slightly to the left, Benchmark to the right\n",
        "    plt.bar(bin_centers - width/2, hist_strat, width=width, label=\"Strategy\")\n",
        "    plt.bar(bin_centers + width/2, hist_bench, width=width, label=\"Benchmark\")\n",
        "    plt.xlabel(\"Daily Returns\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_path)\n",
        "    plt.close()\n",
        "\n",
        "# ---------------------------------\n",
        "# 6) Main Script\n",
        "# ---------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # =================================================\n",
        "    # 1) Load data and do once-only transformations\n",
        "    # =================================================\n",
        "    df_raw = load_data(FILE_NAME)\n",
        "    all_price_cols = SECTOR_PRICE_COLS + [BENCHMARK_PRICE_COL]\n",
        "    df_with_returns = add_daily_return_columns(df_raw, all_price_cols)\n",
        "    sector_daily_ret_cols = [c + \"_daily_ret\" for c in SECTOR_PRICE_COLS]\n",
        "    df_with_fwd = create_180d_forward_returns(df_with_returns, sector_daily_ret_cols, WINDOW)\n",
        "\n",
        "    # Label columns = \"*_daily_ret_fwd180\"\n",
        "    label_cols = [c + \"_fwd180\" for c in sector_daily_ret_cols]\n",
        "\n",
        "    # Scale input features\n",
        "    scaler = MinMaxScaler()\n",
        "    df_scaled = df_with_fwd.copy()\n",
        "    df_scaled[INPUT_COLS] = scaler.fit_transform(df_with_fwd[INPUT_COLS])\n",
        "\n",
        "    # Prepare data for a single LAGS set (the first in LAGS_GRID for example)\n",
        "    chosen_lags = LAGS_GRID[0]\n",
        "    X_full, y_full, dates_full = prepare_lagged_data(\n",
        "        df_scaled, INPUT_COLS, label_cols, chosen_lags\n",
        "    )\n",
        "\n",
        "    precomp_train, precomp_eval = split_three_periods_precomputed_data(\n",
        "        X_full, y_full, dates_full,\n",
        "        TRAIN1_START, TRAIN1_END,\n",
        "        VALIDATION_START, VALIDATION_END,\n",
        "        TRAIN2_START, TRAIN2_END\n",
        "    )\n",
        "    X_train, y_train, train_dates = precomp_train\n",
        "    X_eval, y_eval, eval_dates = precomp_eval\n",
        "\n",
        "    # Build date-filtered subsets for final predictions\n",
        "    RANGE_2001_START = pd.to_datetime(\"2001-01-02\")\n",
        "    RANGE_2001_END   = pd.to_datetime(\"2011-01-03\")\n",
        "    mask_2001_2011 = [(d >= RANGE_2001_START) and (d <= RANGE_2001_END) for d in dates_full]\n",
        "\n",
        "    RANGE_2023_START = pd.to_datetime(\"2021-12-20\")\n",
        "    RANGE_2023_END   = pd.to_datetime(\"2024-12-18\")\n",
        "    mask_2023_2024 = [(d >= RANGE_2023_START) and (d <= RANGE_2023_END) for d in dates_full]\n",
        "\n",
        "    X_2001_2011_full = X_full[mask_2001_2011]\n",
        "    y_2001_2011_full = y_full[mask_2001_2011]\n",
        "    dates_2001_2011_full = [d for d,m in zip(dates_full,mask_2001_2011) if m]\n",
        "\n",
        "    X_2023_2024_full = X_full[mask_2023_2024]\n",
        "    y_2023_2024_full = y_full[mask_2023_2024]\n",
        "    dates_2023_2024_full = [d for d,m in zip(dates_full,mask_2023_2024) if m]\n",
        "\n",
        "    # We'll collect a big list of results for the final \"grid_search_summary.xlsx\"\n",
        "    all_combinations_results = []   # for the portfolio stats\n",
        "    all_combinations_hparams = []   # for hyperparameters\n",
        "\n",
        "    # =================================================\n",
        "    # 2) Nested loop over hyperparameters\n",
        "    # =================================================\n",
        "    for ep in EPOCHS:\n",
        "        for bs in BATCH_SIZE:\n",
        "            for dr in DROPOUT:\n",
        "                for lr in LEARNING_RATE:\n",
        "                    for reg in REG_LAMBDA:\n",
        "                        for n_lstm_layers in NUM_LSTM_LAYERS:\n",
        "                            for first_layer_sz in FIRST_LAYER_SIZE:\n",
        "                                # Create a subfolder for this combination\n",
        "                                combo_name = (\n",
        "                                    f\"combination_E{ep}_B{bs}_Dr{dr}_LR{lr}_Reg{reg}_LSTM{n_lstm_layers}\"\n",
        "                                    f\"_FS{first_layer_sz}\"\n",
        "                                )\n",
        "                                combo_output_dir = os.path.join(MAIN_OUTPUT_DIR, combo_name)\n",
        "                                os.makedirs(combo_output_dir, exist_ok=True)\n",
        "\n",
        "                                # Also create the various subfolders\n",
        "                                folder_25_2001_2011 = os.path.join(combo_output_dir, SUBFOLDER_25pct_2001_2011)\n",
        "                                folder_casc_2001_2011 = os.path.join(combo_output_dir, SUBFOLDER_cascading_2001_2011)\n",
        "                                folder_25_2023_2024 = os.path.join(combo_output_dir, SUBFOLDER_25pct_2023_2024)\n",
        "                                folder_casc_2023_2024 = os.path.join(combo_output_dir, SUBFOLDER_cascading_2023_2024)\n",
        "                                folder_sectors_2001_2011 = os.path.join(combo_output_dir, SUBFOLDER_sectors_2001_2011)\n",
        "                                folder_sectors_2023_2024 = os.path.join(combo_output_dir, SUBFOLDER_sectors_2023_2024)\n",
        "\n",
        "                                for path in [\n",
        "                                    folder_25_2001_2011,\n",
        "                                    folder_casc_2001_2011,\n",
        "                                    folder_25_2023_2024,\n",
        "                                    folder_casc_2023_2024,\n",
        "                                    folder_sectors_2001_2011,\n",
        "                                    folder_sectors_2023_2024\n",
        "                                ]:\n",
        "                                    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "                                # ---------------------------------------------------\n",
        "                                # 2b) Build & Train model\n",
        "                                # ---------------------------------------------------\n",
        "                                n_lags = X_train.shape[1]\n",
        "                                n_features = X_train.shape[2]\n",
        "                                n_outputs = len(label_cols)\n",
        "                                model = build_lstm_model(\n",
        "                                    n_lags=n_lags,\n",
        "                                    n_features=n_features,\n",
        "                                    n_outputs=n_outputs,\n",
        "                                    num_lstm_layers=n_lstm_layers,\n",
        "                                    first_layer_size=first_layer_sz,\n",
        "                                    dropout_rate=dr,\n",
        "                                    reg_lambda=reg,\n",
        "                                    learning_rate=lr\n",
        "                                )\n",
        "\n",
        "                                # Our custom callback for post-15k logic\n",
        "                                best_model_temp = os.path.join(combo_output_dir, \"temp_best_model.keras\")\n",
        "                                post15k_callback = Post15000EarlyStopping(\n",
        "                                    best_model_path=best_model_temp\n",
        "                                )\n",
        "\n",
        "                                # Train\n",
        "                                history = model.fit(\n",
        "                                    X_train, y_train,\n",
        "                                    validation_data=(X_eval, y_eval),\n",
        "                                    epochs=ep,\n",
        "                                    batch_size=bs,\n",
        "                                    verbose=0,\n",
        "                                    callbacks=[post15k_callback]\n",
        "                                )\n",
        "\n",
        "                                # After training, copy the best file to final_lstm_model.keras\n",
        "                                final_model_path = os.path.join(combo_output_dir, \"final_lstm_model.keras\")\n",
        "                                if os.path.exists(best_model_temp):\n",
        "                                    shutil.copyfile(best_model_temp, final_model_path)\n",
        "                                else:\n",
        "                                    # If we never reached epoch 15000 for some reason,\n",
        "                                    # or if no file was created, just save the final model\n",
        "                                    model.save(final_model_path)\n",
        "\n",
        "                                # Plot training vs validation loss\n",
        "                                plt.figure(figsize=(8,5))\n",
        "                                plt.plot(history.history['loss'], label='Train MSE Loss')\n",
        "                                if 'val_loss' in history.history:\n",
        "                                    plt.plot(history.history['val_loss'], label='Validation MSE Loss')\n",
        "                                plt.title(\"Training vs Validation Loss\")\n",
        "                                plt.legend()\n",
        "                                plt.grid(True)\n",
        "                                plt.tight_layout()\n",
        "                                plt.savefig(os.path.join(combo_output_dir, \"training_validation_loss.png\"))\n",
        "                                plt.close()\n",
        "\n",
        "                                # ---------------------------------------------------\n",
        "                                # 2c) Generate predictions with best model\n",
        "                                # ---------------------------------------------------\n",
        "                                loaded_model = load_model(final_model_path)\n",
        "\n",
        "                                y_pred_2001_2011 = loaded_model.predict(X_2001_2011_full)\n",
        "                                y_pred_2023_2024 = loaded_model.predict(X_2023_2024_full)\n",
        "\n",
        "                                # Save sector-level predictions\n",
        "                                def save_sector_predictions_csv_and_plots(\n",
        "                                        dates_eval, y_eval, y_pred_eval, label_cols, out_folder\n",
        "                                ):\n",
        "                                    os.makedirs(out_folder, exist_ok=True)\n",
        "                                    df_pred = pd.DataFrame({\"Date\": dates_eval})\n",
        "                                    df_pred.set_index(\"Date\", inplace=True)\n",
        "                                    for idx, col in enumerate(label_cols):\n",
        "                                        df_pred[f\"Actual_{col}\"] = y_eval[:, idx]\n",
        "                                        df_pred[f\"Pred_{col}\"]   = y_pred_eval[:, idx]\n",
        "\n",
        "                                        # Quick line plot\n",
        "                                        plt.figure(figsize=(10,4))\n",
        "                                        plt.plot(dates_eval, y_eval[:, idx], label=\"Actual\")\n",
        "                                        plt.plot(dates_eval, y_pred_eval[:, idx], label=\"Predicted\", alpha=0.7)\n",
        "                                        plt.title(f\"{col}: Actual vs Predicted\")\n",
        "                                        plt.legend()\n",
        "                                        plt.grid(True)\n",
        "                                        ax = plt.gca()\n",
        "                                        ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
        "                                        ax.xaxis.set_major_formatter(\n",
        "                                            mdates.ConciseDateFormatter(ax.xaxis.get_major_locator())\n",
        "                                        )\n",
        "                                        plt.tight_layout()\n",
        "                                        plt.savefig(os.path.join(out_folder, f\"actual_vs_pred_{col}.png\"))\n",
        "                                        plt.close()\n",
        "\n",
        "                                    df_pred.to_csv(os.path.join(out_folder, \"sector_predictions.csv\"))\n",
        "\n",
        "                                # 2001–2011 predictions\n",
        "                                save_sector_predictions_csv_and_plots(\n",
        "                                    dates_eval=dates_2001_2011_full,\n",
        "                                    y_eval=y_2001_2011_full,\n",
        "                                    y_pred_eval=y_pred_2001_2011,\n",
        "                                    label_cols=label_cols,\n",
        "                                    out_folder=folder_sectors_2001_2011\n",
        "                                )\n",
        "\n",
        "                                # 2023–2024 predictions\n",
        "                                save_sector_predictions_csv_and_plots(\n",
        "                                    dates_eval=dates_2023_2024_full,\n",
        "                                    y_eval=y_2023_2024_full,\n",
        "                                    y_pred_eval=y_pred_2023_2024,\n",
        "                                    label_cols=label_cols,\n",
        "                                    out_folder=folder_sectors_2023_2024\n",
        "                                )\n",
        "\n",
        "                                # ---------------------------------------------------\n",
        "                                # 2d) Run the four portfolios\n",
        "                                # ---------------------------------------------------\n",
        "                                df_2001_2011 = df_with_returns.loc[\n",
        "                                    RANGE_2001_START:RANGE_2001_END,\n",
        "                                    sector_daily_ret_cols + [BENCHMARK_PRICE_COL + \"_daily_ret\"]\n",
        "                                ]\n",
        "                                df_2023_2024 = df_with_returns.loc[\n",
        "                                    RANGE_2023_START:RANGE_2023_END,\n",
        "                                    sector_daily_ret_cols + [BENCHMARK_PRICE_COL + \"_daily_ret\"]\n",
        "                                ]\n",
        "\n",
        "                                # (A1) 25% each, 2001–2011\n",
        "                                nav_25_2001_2011, top4_25_2001_2011 = backtest_top4_portfolio(\n",
        "                                    dates_eval=dates_2001_2011_full,\n",
        "                                    y_pred=y_pred_2001_2011,\n",
        "                                    df_eval=df_2001_2011,\n",
        "                                    sector_daily_ret_cols=sector_daily_ret_cols,\n",
        "                                    benchmark_daily_ret_col=BENCHMARK_PRICE_COL + \"_daily_ret\",\n",
        "                                    rebal_freq=REBALANCE_FREQUENCY_DAYS\n",
        "                                )\n",
        "                                stats_25_2001_2011 = compute_performance(nav_25_2001_2011[\"Strategy_NAV\"])\n",
        "\n",
        "                                # Compute daily returns\n",
        "                                strat_daily_rets_25_2001_2011 = nav_25_2001_2011[\"Strategy_NAV\"].pct_change().dropna()\n",
        "                                bench_daily_rets_25_2001_2011 = nav_25_2001_2011[\"Benchmark_NAV\"].pct_change().dropna()\n",
        "\n",
        "                                # Add skew/kurt\n",
        "                                stats_25_2001_2011[\"Strategy_Skewness\"] = strat_daily_rets_25_2001_2011.skew()\n",
        "                                stats_25_2001_2011[\"Strategy_Kurtosis\"] = strat_daily_rets_25_2001_2011.kurt()\n",
        "                                stats_25_2001_2011[\"Benchmark_Skewness\"] = bench_daily_rets_25_2001_2011.skew()\n",
        "                                stats_25_2001_2011[\"Benchmark_Kurtosis\"] = bench_daily_rets_25_2001_2011.kurt()\n",
        "\n",
        "                                # NEW: Calculate Beta = Cov(strategy, benchmark) / Var(benchmark)\n",
        "                                cov = np.cov(strat_daily_rets_25_2001_2011, bench_daily_rets_25_2001_2011)[0,1]\n",
        "                                var_bench = np.var(bench_daily_rets_25_2001_2011)\n",
        "                                beta_25_2001_2011 = cov / var_bench if var_bench != 0 else np.nan\n",
        "                                stats_25_2001_2011[\"Strategy_Beta\"] = beta_25_2001_2011\n",
        "\n",
        "                                # Plot NAV\n",
        "                                plt.figure(figsize=(10,5))\n",
        "                                plt.plot(nav_25_2001_2011.index, nav_25_2001_2011[\"Strategy_NAV\"], label=\"Strategy\")\n",
        "                                plt.plot(nav_25_2001_2011.index, nav_25_2001_2011[\"Benchmark_NAV\"], label=\"Benchmark\")\n",
        "                                plt.title(\"25% Top-4 (2001–2011)\")\n",
        "                                plt.legend()\n",
        "                                plt.grid(True)\n",
        "                                ax = plt.gca()\n",
        "                                ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
        "                                ax.xaxis.set_major_formatter(mdates.ConciseDateFormatter(ax.xaxis.get_major_locator()))\n",
        "                                plt.tight_layout()\n",
        "                                plt.savefig(os.path.join(folder_25_2001_2011, \"NAV_comparison.png\"))\n",
        "                                plt.close()\n",
        "                                plot_drawdowns(nav_25_2001_2011,\n",
        "                                               os.path.join(folder_25_2001_2011, \"drawdowns.png\"))\n",
        "                                plot_top4_heatmap(dates_2001_2011_full,\n",
        "                                                  top4_25_2001_2011,\n",
        "                                                  sector_daily_ret_cols,\n",
        "                                                  os.path.join(folder_25_2001_2011, \"top4_heatmap.png\"))\n",
        "                                plot_side_by_side_hist(\n",
        "                                    strat_daily_rets_25_2001_2011,\n",
        "                                    bench_daily_rets_25_2001_2011,\n",
        "                                    \"Distribution of Daily Returns (25% Top-4 2001–2011)\",\n",
        "                                    os.path.join(folder_25_2001_2011, \"hist_strategy_benchmark.png\")\n",
        "                                )\n",
        "\n",
        "                                # (A2) Cascading, 2001–2011\n",
        "                                nav_casc_2001_2011, w_casc_2001_2011 = backtest_top4_cascading_weights(\n",
        "                                    dates_eval=dates_2001_2011_full,\n",
        "                                    y_pred=y_pred_2001_2011,\n",
        "                                    df_eval=df_2001_2011,\n",
        "                                    sector_daily_ret_cols=sector_daily_ret_cols,\n",
        "                                    benchmark_daily_ret_col=BENCHMARK_PRICE_COL + \"_daily_ret\",\n",
        "                                    rebal_freq=REBALANCE_FREQUENCY_DAYS\n",
        "                                )\n",
        "                                stats_casc_2001_2011 = compute_performance(nav_casc_2001_2011[\"Strategy_NAV\"])\n",
        "\n",
        "                                strat_daily_rets_casc_2001_2011 = nav_casc_2001_2011[\"Strategy_NAV\"].pct_change().dropna()\n",
        "                                bench_daily_rets_casc_2001_2011 = nav_casc_2001_2011[\"Benchmark_NAV\"].pct_change().dropna()\n",
        "\n",
        "                                stats_casc_2001_2011[\"Strategy_Skewness\"] = strat_daily_rets_casc_2001_2011.skew()\n",
        "                                stats_casc_2001_2011[\"Strategy_Kurtosis\"] = strat_daily_rets_casc_2001_2011.kurt()\n",
        "                                stats_casc_2001_2011[\"Benchmark_Skewness\"] = bench_daily_rets_casc_2001_2011.skew()\n",
        "                                stats_casc_2001_2011[\"Benchmark_Kurtosis\"] = bench_daily_rets_casc_2001_2011.kurt()\n",
        "\n",
        "                                cov_casc = np.cov(strat_daily_rets_casc_2001_2011, bench_daily_rets_casc_2001_2011)[0,1]\n",
        "                                var_bench_casc = np.var(bench_daily_rets_casc_2001_2011)\n",
        "                                beta_casc_2001_2011 = cov_casc / var_bench_casc if var_bench_casc != 0 else np.nan\n",
        "                                stats_casc_2001_2011[\"Strategy_Beta\"] = beta_casc_2001_2011\n",
        "\n",
        "                                plt.figure(figsize=(10,5))\n",
        "                                plt.plot(nav_casc_2001_2011.index, nav_casc_2001_2011[\"Strategy_NAV\"], label=\"Strategy\")\n",
        "                                plt.plot(nav_casc_2001_2011.index, nav_casc_2001_2011[\"Benchmark_NAV\"], label=\"Benchmark\")\n",
        "                                plt.title(\"Cascading Weights Top-4 (2001–2011)\")\n",
        "                                plt.legend()\n",
        "                                plt.grid(True)\n",
        "                                ax = plt.gca()\n",
        "                                ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
        "                                ax.xaxis.set_major_formatter(mdates.ConciseDateFormatter(ax.xaxis.get_major_locator()))\n",
        "                                plt.tight_layout()\n",
        "                                plt.savefig(os.path.join(folder_casc_2001_2011, \"NAV_comparison.png\"))\n",
        "                                plt.close()\n",
        "                                plot_drawdowns(nav_casc_2001_2011,\n",
        "                                               os.path.join(folder_casc_2001_2011, \"drawdowns.png\"))\n",
        "                                plot_cascading_weight_heatmap(dates_2001_2011_full,\n",
        "                                                              w_casc_2001_2011,\n",
        "                                                              sector_daily_ret_cols,\n",
        "                                                              os.path.join(folder_casc_2001_2011, \"cascading_heatmap.png\"))\n",
        "                                plot_side_by_side_hist(\n",
        "                                    strat_daily_rets_casc_2001_2011,\n",
        "                                    bench_daily_rets_casc_2001_2011,\n",
        "                                    \"Distribution of Daily Returns (Cascading Top-4 2001–2011)\",\n",
        "                                    os.path.join(folder_casc_2001_2011, \"hist_strategy_benchmark.png\")\n",
        "                                )\n",
        "\n",
        "                                # (B1) 25% each, 2023–2024\n",
        "                                nav_25_2023_2024, top4_25_2023_2024 = backtest_top4_portfolio(\n",
        "                                    dates_eval=dates_2023_2024_full,\n",
        "                                    y_pred=y_pred_2023_2024,\n",
        "                                    df_eval=df_2023_2024,\n",
        "                                    sector_daily_ret_cols=sector_daily_ret_cols,\n",
        "                                    benchmark_daily_ret_col=BENCHMARK_PRICE_COL + \"_daily_ret\",\n",
        "                                    rebal_freq=REBALANCE_FREQUENCY_DAYS\n",
        "                                )\n",
        "                                stats_25_2023_2024 = compute_performance(nav_25_2023_2024[\"Strategy_NAV\"])\n",
        "\n",
        "                                strat_daily_rets_25_2023_2024 = nav_25_2023_2024[\"Strategy_NAV\"].pct_change().dropna()\n",
        "                                bench_daily_rets_25_2023_2024 = nav_25_2023_2024[\"Benchmark_NAV\"].pct_change().dropna()\n",
        "\n",
        "                                stats_25_2023_2024[\"Strategy_Skewness\"] = strat_daily_rets_25_2023_2024.skew()\n",
        "                                stats_25_2023_2024[\"Strategy_Kurtosis\"] = strat_daily_rets_25_2023_2024.kurt()\n",
        "                                stats_25_2023_2024[\"Benchmark_Skewness\"] = bench_daily_rets_25_2023_2024.skew()\n",
        "                                stats_25_2023_2024[\"Benchmark_Kurtosis\"] = bench_daily_rets_25_2023_2024.kurt()\n",
        "\n",
        "                                cov_25_2023_2024 = np.cov(strat_daily_rets_25_2023_2024, bench_daily_rets_25_2023_2024)[0,1]\n",
        "                                var_bench_25_2023_2024 = np.var(bench_daily_rets_25_2023_2024)\n",
        "                                beta_25_2023_2024 = cov_25_2023_2024 / var_bench_25_2023_2024 if var_bench_25_2023_2024!=0 else np.nan\n",
        "                                stats_25_2023_2024[\"Strategy_Beta\"] = beta_25_2023_2024\n",
        "\n",
        "                                plt.figure(figsize=(10,5))\n",
        "                                plt.plot(nav_25_2023_2024.index, nav_25_2023_2024[\"Strategy_NAV\"], label=\"Strategy\")\n",
        "                                plt.plot(nav_25_2023_2024.index, nav_25_2023_2024[\"Benchmark_NAV\"], label=\"Benchmark\")\n",
        "                                plt.title(\"25% Top-4 (2023–2024)\")\n",
        "                                plt.legend()\n",
        "                                plt.grid(True)\n",
        "                                ax = plt.gca()\n",
        "                                ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
        "                                ax.xaxis.set_major_formatter(mdates.ConciseDateFormatter(ax.xaxis.get_major_locator()))\n",
        "                                plt.tight_layout()\n",
        "                                plt.savefig(os.path.join(folder_25_2023_2024, \"NAV_comparison.png\"))\n",
        "                                plt.close()\n",
        "                                plot_drawdowns(nav_25_2023_2024,\n",
        "                                               os.path.join(folder_25_2023_2024, \"drawdowns.png\"))\n",
        "                                plot_top4_heatmap(dates_2023_2024_full,\n",
        "                                                  top4_25_2023_2024,\n",
        "                                                  sector_daily_ret_cols,\n",
        "                                                  os.path.join(folder_25_2023_2024, \"top4_heatmap.png\"))\n",
        "                                plot_side_by_side_hist(\n",
        "                                    strat_daily_rets_25_2023_2024,\n",
        "                                    bench_daily_rets_25_2023_2024,\n",
        "                                    \"Distribution of Daily Returns (25% Top-4 2023–2024)\",\n",
        "                                    os.path.join(folder_25_2023_2024, \"hist_strategy_benchmark.png\")\n",
        "                                )\n",
        "\n",
        "                                # (B2) Cascading, 2023–2024\n",
        "                                nav_casc_2023_2024, w_casc_2023_2024 = backtest_top4_cascading_weights(\n",
        "                                    dates_eval=dates_2023_2024_full,\n",
        "                                    y_pred=y_pred_2023_2024,\n",
        "                                    df_eval=df_2023_2024,\n",
        "                                    sector_daily_ret_cols=sector_daily_ret_cols,\n",
        "                                    benchmark_daily_ret_col=BENCHMARK_PRICE_COL + \"_daily_ret\",\n",
        "                                    rebal_freq=REBALANCE_FREQUENCY_DAYS\n",
        "                                )\n",
        "                                stats_casc_2023_2024 = compute_performance(nav_casc_2023_2024[\"Strategy_NAV\"])\n",
        "\n",
        "                                strat_daily_rets_casc_2023_2024 = nav_casc_2023_2024[\"Strategy_NAV\"].pct_change().dropna()\n",
        "                                bench_daily_rets_casc_2023_2024 = nav_casc_2023_2024[\"Benchmark_NAV\"].pct_change().dropna()\n",
        "\n",
        "                                stats_casc_2023_2024[\"Strategy_Skewness\"] = strat_daily_rets_casc_2023_2024.skew()\n",
        "                                stats_casc_2023_2024[\"Strategy_Kurtosis\"] = strat_daily_rets_casc_2023_2024.kurt()\n",
        "                                stats_casc_2023_2024[\"Benchmark_Skewness\"] = bench_daily_rets_casc_2023_2024.skew()\n",
        "                                stats_casc_2023_2024[\"Benchmark_Kurtosis\"] = bench_daily_rets_casc_2023_2024.kurt()\n",
        "\n",
        "                                cov_casc_2023_2024 = np.cov(strat_daily_rets_casc_2023_2024, bench_daily_rets_casc_2023_2024)[0,1]\n",
        "                                var_bench_casc_2023_2024 = np.var(bench_daily_rets_casc_2023_2024)\n",
        "                                beta_casc_2023_2024 = cov_casc_2023_2024 / var_bench_casc_2023_2024 if var_bench_casc_2023_2024!=0 else np.nan\n",
        "                                stats_casc_2023_2024[\"Strategy_Beta\"] = beta_casc_2023_2024\n",
        "\n",
        "                                plt.figure(figsize=(10,5))\n",
        "                                plt.plot(nav_casc_2023_2024.index, nav_casc_2023_2024[\"Strategy_NAV\"], label=\"Strategy\")\n",
        "                                plt.plot(nav_casc_2023_2024.index, nav_casc_2023_2024[\"Benchmark_NAV\"], label=\"Benchmark\")\n",
        "                                plt.title(\"Cascading Weights Top-4 (2023–2024)\")\n",
        "                                plt.legend()\n",
        "                                plt.grid(True)\n",
        "                                ax = plt.gca()\n",
        "                                ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
        "                                ax.xaxis.set_major_formatter(mdates.ConciseDateFormatter(ax.xaxis.get_major_locator()))\n",
        "                                plt.tight_layout()\n",
        "                                plt.savefig(os.path.join(folder_casc_2023_2024, \"NAV_comparison.png\"))\n",
        "                                plt.close()\n",
        "                                plot_drawdowns(nav_casc_2023_2024,\n",
        "                                               os.path.join(folder_casc_2023_2024, \"drawdowns.png\"))\n",
        "                                plot_cascading_weight_heatmap(dates_2023_2024_full,\n",
        "                                                              w_casc_2023_2024,\n",
        "                                                              sector_daily_ret_cols,\n",
        "                                                              os.path.join(folder_casc_2023_2024, \"cascading_heatmap.png\"))\n",
        "                                plot_side_by_side_hist(\n",
        "                                    strat_daily_rets_casc_2023_2024,\n",
        "                                    bench_daily_rets_casc_2023_2024,\n",
        "                                    \"Distribution of Daily Returns (Cascading Top-4 2023–2024)\",\n",
        "                                    os.path.join(folder_casc_2023_2024, \"hist_strategy_benchmark.png\")\n",
        "                                )\n",
        "\n",
        "                                # ---------------------------------------------------\n",
        "                                # 2e) Consolidate results into one Excel\n",
        "                                # ---------------------------------------------------\n",
        "                                results_data = []\n",
        "                                results_data.append({\"Scenario\": \"25pct_2001_2011\", **stats_25_2001_2011})\n",
        "                                results_data.append({\"Scenario\": \"cascading_2001_2011\", **stats_casc_2001_2011})\n",
        "                                results_data.append({\"Scenario\": \"25pct_2023_2024\", **stats_25_2023_2024})\n",
        "                                results_data.append({\"Scenario\": \"cascading_2023_2024\", **stats_casc_2023_2024})\n",
        "                                results_df = pd.DataFrame(results_data)\n",
        "\n",
        "                                # Save hyperparams to a separate sheet\n",
        "                                hyperparams_data = [\n",
        "                                    {\n",
        "                                        \"EPOCHS\": ep,\n",
        "                                        \"BATCH_SIZE\": bs,\n",
        "                                        \"DROPOUT\": dr,\n",
        "                                        \"LEARNING_RATE\": lr,\n",
        "                                        \"REG_LAMBDA\": reg,\n",
        "                                        \"NUM_LSTM_LAYERS\": n_lstm_layers,\n",
        "                                        \"FIRST_LAYER_SIZE\": first_layer_sz\n",
        "                                    }\n",
        "                                ]\n",
        "                                hyperparams_df = pd.DataFrame(hyperparams_data)\n",
        "\n",
        "                                excel_path = os.path.join(combo_output_dir, \"all_portfolios_results.xlsx\")\n",
        "                                with pd.ExcelWriter(excel_path) as writer:\n",
        "                                    results_df.to_excel(writer, sheet_name=\"Portfolio_Results\", index=False)\n",
        "                                    hyperparams_df.to_excel(writer, sheet_name=\"Hyperparameters\", index=False)\n",
        "\n",
        "                                # We'll also store these results in a global list\n",
        "                                for row in results_data:\n",
        "                                    row[\"EPOCHS\"] = ep\n",
        "                                    row[\"BATCH_SIZE\"] = bs\n",
        "                                    row[\"DROPOUT\"] = dr\n",
        "                                    row[\"LEARNING_RATE\"] = lr\n",
        "                                    row[\"REG_LAMBDA\"] = reg\n",
        "                                    row[\"NUM_LSTM_LAYERS\"] = n_lstm_layers\n",
        "                                    row[\"FIRST_LAYER_SIZE\"] = first_layer_sz\n",
        "                                    all_combinations_results.append(row)\n",
        "\n",
        "                                # Also store a single row (with no scenario) in the hyperparams summary\n",
        "                                all_combinations_hparams.append({\n",
        "                                    \"EPOCHS\": ep,\n",
        "                                    \"BATCH_SIZE\": bs,\n",
        "                                    \"DROPOUT\": dr,\n",
        "                                    \"LEARNING_RATE\": lr,\n",
        "                                    \"REG_LAMBDA\": reg,\n",
        "                                    \"NUM_LSTM_LAYERS\": n_lstm_layers,\n",
        "                                    \"FIRST_LAYER_SIZE\": first_layer_sz\n",
        "                                })\n",
        "\n",
        "                                # ---------------------------------------------------\n",
        "                                # 2f) (Optional) Zip up just this combination\n",
        "                                # ---------------------------------------------------\n",
        "                                zip_path_combo = os.path.join(combo_output_dir, f\"{combo_name}.zip\")\n",
        "                                with zipfile.ZipFile(zip_path_combo, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "                                    for root, dirs, files in os.walk(combo_output_dir):\n",
        "                                        # Don't recursively add the zip inside itself\n",
        "                                        for file in files:\n",
        "                                            if file == f\"{combo_name}.zip\":\n",
        "                                                continue\n",
        "                                            file_path = os.path.join(root, file)\n",
        "                                            arcname = os.path.relpath(file_path, start=combo_output_dir)\n",
        "                                            zipf.write(file_path, arcname)\n",
        "\n",
        "    # ================================================================\n",
        "    # 3) After finishing ALL combos, create a big summary in MAIN_OUTPUT_DIR\n",
        "    # ================================================================\n",
        "    master_summary_path = os.path.join(MAIN_OUTPUT_DIR, \"grid_search_summary.xlsx\")\n",
        "\n",
        "    all_results_df = pd.DataFrame(all_combinations_results)\n",
        "    all_hparams_df = pd.DataFrame(all_combinations_hparams).drop_duplicates()\n",
        "\n",
        "    with pd.ExcelWriter(master_summary_path) as writer:\n",
        "        all_results_df.to_excel(writer, sheet_name=\"All_Combos_Portfolio_Results\", index=False)\n",
        "        all_hparams_df.to_excel(writer, sheet_name=\"All_Combos_Hyperparameters\", index=False)\n",
        "\n",
        "    # ================================================================\n",
        "    # 4) (Optional) Zip the entire MAIN_OUTPUT_DIR if desired\n",
        "    # ================================================================\n",
        "    zip_path = \"grid_search_results_LSTM.zip\"\n",
        "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, dirs, files in os.walk(MAIN_OUTPUT_DIR):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                arcname = os.path.relpath(file_path, start=MAIN_OUTPUT_DIR)\n",
        "                zipf.write(file_path, arcname)\n",
        "    print(f\"All results have been zipped into: {zip_path}\")\n",
        "\n",
        "    # Attempt to download if in Colab (optional)\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        files.download(zip_path)\n",
        "    except ImportError:\n",
        "        pass\n",
        "\n",
        "    print(\"All hyperparameter combinations are complete.\")\n",
        "    print(f\"Master summary saved to: {master_summary_path}\")\n",
        "    print(f\"Zipped folder created at: {zip_path}\")\n",
        "    print(\"Script complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "1xqRq5QthB49",
        "outputId": "022eef5c-5b4a-441a-8c7d-0e571e9751d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All results have been zipped into: grid_search_results_LSTM.zip\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c734d043-6d66-40ce-8f63-3fafe696ed06\", \"grid_search_results_LSTM.zip\", 817971687)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "    zip_path = \"grid_search_results_LSTM.zip\"\n",
        "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, dirs, files in os.walk(MAIN_OUTPUT_DIR):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                arcname = os.path.relpath(file_path, start=MAIN_OUTPUT_DIR)\n",
        "                zipf.write(file_path, arcname)\n",
        "    print(f\"All results have been zipped into: {zip_path}\")\n",
        "\n",
        "    # Attempt to download if in Colab (optional)\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        files.download(zip_path)\n",
        "    except ImportError:\n",
        "        pass"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMXOgMjObBogU1nqr0NGous",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}